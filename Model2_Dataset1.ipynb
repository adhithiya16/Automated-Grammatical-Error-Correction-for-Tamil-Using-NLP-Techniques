{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1599521,"sourceType":"datasetVersion","datasetId":943922}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Load your dataset\nfile_path = \"/kaggle/input/error-annotated-tamil-corpus/Error Annotated Corpus.csv\"  # Update with your actual file path\ndf = pd.read_csv(file_path)\n\n# Check the first few rows to understand the structure\nprint(df.head())\n\n# Remove rows with any null values in the dataset\ndf_cleaned = df.dropna()\n\n# Remove duplicate rows\ndf_cleaned = df_cleaned.drop_duplicates()\n\n# Optionally, you can reset the index after cleaning\ndf_cleaned = df_cleaned.reset_index(drop=True)\n\n# Check the cleaned dataset\nprint(df_cleaned.head())\n\n# Save the cleaned dataset to a new CSV file\ndf_cleaned.to_csv(\"cleaned_dataset.csv\", index=False)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:38:34.188596Z","iopub.execute_input":"2025-03-13T12:38:34.189064Z","iopub.status.idle":"2025-03-13T12:38:34.248763Z","shell.execute_reply.started":"2025-03-13T12:38:34.189026Z","shell.execute_reply":"2025-03-13T12:38:34.247572Z"}},"outputs":[{"name":"stdout","text":"  Error word & consecutive word     Corrected words & its    Annotation\n0               10கனநீர் உலைகள்           10கணநீர் உலைகள்  வேற்றெழுத்து\n1                   அகளவிரித்து               அகலவிரித்து  வேற்றெழுத்து\n2      அணைத்து ஊழியர்களுக்குமான  அனைத்து ஊழியர்களுக்குமான  வேற்றெழுத்து\n3               அதள பாதாளதுக்கு         அதல பாதாளத்துக்கு  வேற்றெழுத்து\n4           அதற்குறிய தீர்வுகளை       அதற்குரிய தீர்வுகளை  வேற்றெழுத்து\n  Error word & consecutive word     Corrected words & its    Annotation\n0               10கனநீர் உலைகள்           10கணநீர் உலைகள்  வேற்றெழுத்து\n1                   அகளவிரித்து               அகலவிரித்து  வேற்றெழுத்து\n2      அணைத்து ஊழியர்களுக்குமான  அனைத்து ஊழியர்களுக்குமான  வேற்றெழுத்து\n3               அதள பாதாளதுக்கு         அதல பாதாளத்துக்கு  வேற்றெழுத்து\n4           அதற்குறிய தீர்வுகளை       அதற்குரிய தீர்வுகளை  வேற்றெழுத்து\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(df_cleaned.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:38:34.250245Z","iopub.execute_input":"2025-03-13T12:38:34.250689Z","iopub.status.idle":"2025-03-13T12:38:34.255616Z","shell.execute_reply.started":"2025-03-13T12:38:34.250573Z","shell.execute_reply":"2025-03-13T12:38:34.254440Z"}},"outputs":[{"name":"stdout","text":"Index(['Error word & consecutive word', 'Corrected words & its', 'Annotation'], dtype='object')\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,AutoModel\nimport pandas as pd\n\n# Load Tamil-specific model and tokenizer\nmodel_name = \"facebook/nllb-200-distilled-600M\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\n# Load your cleaned dataset\ndf_cleaned = pd.read_csv(\"cleaned_dataset.csv\")\n\n# Calculate max input and target lengths based on tokenized sentences\nmax_input_length = max([len(tokenizer.encode(sentence, truncation=True, padding=False)) for sentence in df_cleaned['Error word & consecutive word']])\nmax_target_length = max([len(tokenizer.encode(sentence, truncation=True, padding=False)) for sentence in df_cleaned['Corrected words & its']])\n\nprint(f\"Max Input Length: {max_input_length}\")\nprint(f\"Max Target Length: {max_target_length}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:38:34.257960Z","iopub.execute_input":"2025-03-13T12:38:34.258314Z","iopub.status.idle":"2025-03-13T12:39:29.264181Z","shell.execute_reply.started":"2025-03-13T12:38:34.258289Z","shell.execute_reply":"2025-03-13T12:39:29.262741Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d61f0340f3b04fd9a5d367a8a8c4776e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"508f50e2b2904887a6cb1f304ccc008a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f48c1f483a284132bb7af2a716a94ccf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44873f4f768d4378bc53733ad383a726"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdeb8981bc6e4dd088b04d05be96f9b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"313ede583c8c41d4a1b27c88baacf2d3"}},"metadata":{}},{"name":"stdout","text":"Max Input Length: 17\nMax Target Length: 27\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_MODE\"] = \"disabled\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:39:29.266622Z","iopub.execute_input":"2025-03-13T12:39:29.267066Z","iopub.status.idle":"2025-03-13T12:39:29.272283Z","shell.execute_reply.started":"2025-03-13T12:39:29.267021Z","shell.execute_reply":"2025-03-13T12:39:29.271242Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\n\n# Load the cleaned dataset\ndf_cleaned = pd.read_csv(\"cleaned_dataset.csv\")\n\n# Split the dataset into training and validation sets\ntrain_df, val_df = train_test_split(df_cleaned, test_size=0.1)\n\n# Convert the dataframe to Hugging Face dataset format\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Load NLLB-200 model and tokenizer\nmodel_name = \"facebook/nllb-200-distilled-600M\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Set the language code for Tamil\ntamil_lang_code = \"tam_Taml\"\n\n# Tokenizer function\ndef preprocess_function(examples):\n    inputs = examples['Error word & consecutive word']\n    targets = examples['Corrected words & its']\n\n    # Add language token for Tamil\n    inputs = [f\"{tamil_lang_code} {text}\" for text in inputs]\n    \n    # Tokenize input and target sequences\n    model_inputs = tokenizer(inputs, max_length=17, truncation=True, padding=\"max_length\")\n    labels = tokenizer(targets, max_length=27, truncation=True, padding=\"max_length\")\n\n    # Add the labels to model inputs\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    return model_inputs\n\n# Apply tokenization\ntrain_dataset = train_dataset.map(preprocess_function, batched=True)\nval_dataset = val_dataset.map(preprocess_function, batched=True)\n\n# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    warmup_steps=int(0.1 * len(train_dataset)),\n    lr_scheduler_type=\"linear\",\n    gradient_accumulation_steps=2,\n    max_grad_norm=1.0,\n    run_name=\"tamil-error-correction-nllb200\",\n    report_to=[]\n)\n\n# Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer\n)\n\n# Train the model\ntrainer.train()\n\n# Save the trained model\ntrainer.save_model(\"trained_model_nllb200\")\n\n# Evaluate the model\nresults = trainer.evaluate(val_dataset)\nprint(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:39:29.273627Z","iopub.execute_input":"2025-03-13T12:39:29.273958Z","iopub.status.idle":"2025-03-13T13:03:30.092480Z","shell.execute_reply.started":"2025-03-13T12:39:29.273931Z","shell.execute_reply":"2025-03-13T13:03:30.091320Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc8a1fa9d38b4fad8df261c1aae15f0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4510 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86230b270e134d5caea78c7594838b09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/502 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b35fe8506b54c3d99f5a8fc4a2d1bf2"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-10-1d6d0c405fd0>:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='705' max='705' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [705/705 23:22, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>7.995195</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>4.742764</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>2.586726</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>6.043500</td>\n      <td>0.766727</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>6.043500</td>\n      <td>0.409859</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [32/32 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.4098591208457947, 'eval_runtime': 11.6067, 'eval_samples_per_second': 43.251, 'eval_steps_per_second': 2.757, 'epoch': 5.0}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract logs from Trainer state\nlog_history = trainer.state.log_history\n\n# Separate training and validation losses\ntrain_losses = [entry['loss'] for entry in log_history if 'loss' in entry]\neval_losses = [entry['eval_loss'] for entry in log_history if 'eval_loss' in entry]\nlearning_rates = [entry['learning_rate'] for entry in log_history if 'learning_rate' in entry]\n\n# Create x-axis for epochs\n\nepochs_eval = range(1, len(eval_losses) + 1)\n\n\n# Plot validation loss\nplt.figure(figsize=(6, 4))\nplt.plot(epochs_eval, eval_losses, label=\"Validation Loss\", marker='o', color='red')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Validation Loss over Epochs\")\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:03:30.094107Z","iopub.execute_input":"2025-03-13T13:03:30.094515Z","iopub.status.idle":"2025-03-13T13:03:30.841761Z","shell.execute_reply.started":"2025-03-13T13:03:30.094476Z","shell.execute_reply":"2025-03-13T13:03:30.840647Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom nltk.translate.bleu_score import sentence_bleu\n\n\n# Check if GPU is available and move the model to GPU if it is\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\n\n# Generate predictions on the validation set\npredictions = []\nreferences = []\n\n# Use the model to generate predictions on the validation set\nfor example in val_dataset:\n    input_ids = tokenizer(example['Error word & consecutive word'], return_tensors='pt', padding=True, truncation=True, max_length=35).input_ids\n    \n    # Move input_ids to the correct device (same as the model)\n    input_ids = input_ids.to(device)\n    \n    output_ids = model.generate(input_ids)\n    decoded_prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)  # Use tokenizer's decode method\n\n    predictions.append(decoded_prediction)\n    references.append(example['Corrected words & its'])\n\n# BLEU Score Calculation (with n-gram overlap)\ndef calculate_bleu(predictions, references):\n    bleu_scores = []\n    for pred, ref in zip(predictions, references):\n        # Tokenize sentences and calculate BLEU score\n        pred_tokens = pred.split()\n        ref_tokens = ref.split()\n        bleu_scores.append(sentence_bleu([ref_tokens], pred_tokens))  # List of references is passed to sentence_bleu\n    return sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n\n\n# Calculate BLEU\nbleu_score = calculate_bleu(predictions, references)\n\n\n# Print the results\nprint(f\"BLEU Score: {bleu_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:03:30.842790Z","iopub.execute_input":"2025-03-13T13:03:30.843072Z","iopub.status.idle":"2025-03-13T13:04:38.825388Z","shell.execute_reply.started":"2025-03-13T13:03:30.843049Z","shell.execute_reply":"2025-03-13T13:04:38.824286Z"}},"outputs":[{"name":"stdout","text":"BLEU Score: 0.7861\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install bert_score\nfrom bert_score import score\nP, R, F1 = score(predictions, references, lang=\"ta\")  # Adjust the language if necessary (e.g., \"ta\" for Tamil)\n\n# Print BERT scores\nprint(f\"BERT Precision: {P.mean():.4f}\")\nprint(f\"BERT Recall: {R.mean():.4f}\")\nprint(f\"BERT F1 Score: {F1.mean():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:04:38.826354Z","iopub.execute_input":"2025-03-13T13:04:38.826611Z","iopub.status.idle":"2025-03-13T13:04:54.479388Z","shell.execute_reply.started":"2025-03-13T13:04:38.826578Z","shell.execute_reply":"2025-03-13T13:04:54.478213Z"}},"outputs":[{"name":"stdout","text":"Collecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.47.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.29.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bert_score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db6315cfd770407ba161d9e85e52a7f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62218ab4c857457e96342db476e4b82e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eea90636804946d28a5cce4411662f08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65f4151792114962a5d3147352cfa562"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f62d16f99d1e4a91b90bcf5c92de0b4f"}},"metadata":{}},{"name":"stdout","text":"BERT Precision: 0.9575\nBERT Recall: 0.9582\nBERT F1 Score: 0.9578\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import nltk\n\ndef calculate_ter(references, predictions):\n    \"\"\"\n    Compute TER (Translation Edit Rate) score.\n    references: list of reference sentences (the correct sentences)\n    predictions: list of generated sentences (the sentences predicted by the model)\n    \"\"\"\n    # Initialize the TER scores\n    ter_scores = []\n    \n    # Loop over all the sentences\n    for ref, hyp in zip(references, predictions):\n        ref_tokens = ref.split()  # Tokenize the reference sentence\n        hyp_tokens = hyp.split()  # Tokenize the hypothesis sentence\n\n        # Find the minimum edit distance (substitutions, insertions, deletions)\n        edits = nltk.edit_distance(ref_tokens, hyp_tokens)\n\n        # TER is calculated as (number of edits + len(reference) - len(hypothesis)) / len(reference)\n        ter = (edits + len(ref_tokens) - len(hyp_tokens)) / len(ref_tokens)\n        ter_scores.append(ter)\n\n    # Average TER score across all sentences\n    avg_ter = sum(ter_scores) / len(ter_scores) if ter_scores else 0\n    return avg_ter\n\n\n\nter_score = calculate_ter(references, predictions)\nprint(f\"TER Score: {ter_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:04:54.481860Z","iopub.execute_input":"2025-03-13T13:04:54.482159Z","iopub.status.idle":"2025-03-13T13:04:54.492935Z","shell.execute_reply.started":"2025-03-13T13:04:54.482119Z","shell.execute_reply":"2025-03-13T13:04:54.492092Z"}},"outputs":[{"name":"stdout","text":"TER Score: 0.3493\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import nltk\nfrom collections import Counter\n\ndef calculate_gleu(references, predictions, max_order=4):\n    \"\"\"\n    Compute GLEU score.\n    references: list of reference sentences (the correct sentences)\n    predictions: list of generated sentences (the sentences predicted by the model)\n    max_order: maximum n-gram length for the BLEU calculation (default is 4)\n    \"\"\"\n    def get_ngrams(tokens, n):\n        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n\n    def compute_sentence_gleu(ref_tokens, hyp_tokens):\n        \"\"\"\n        Compute the GLEU score for a single sentence pair.\n        \"\"\"\n        total_ref_ngrams = Counter()\n        total_hyp_ngrams = Counter()\n        total_match_ngrams = Counter()\n\n        for n in range(1, max_order + 1):\n            ref_ngrams = get_ngrams(ref_tokens, n)\n            hyp_ngrams = get_ngrams(hyp_tokens, n)\n            \n            ref_ngrams_count = Counter(ref_ngrams)\n            hyp_ngrams_count = Counter(hyp_ngrams)\n\n            total_ref_ngrams.update(ref_ngrams_count)\n            total_hyp_ngrams.update(hyp_ngrams_count)\n            total_match_ngrams.update(ref_ngrams_count & hyp_ngrams_count)  # Intersection for matching n-grams\n\n        # Compute precision and recall\n        precision = sum(total_match_ngrams.values()) / sum(total_hyp_ngrams.values()) if total_hyp_ngrams else 0\n        recall = sum(total_match_ngrams.values()) / sum(total_ref_ngrams.values()) if total_ref_ngrams else 0\n\n        # Compute GLEU score (harmonic mean of precision and recall)\n        if precision + recall > 0:\n            gleu = (1 + 0.5) * (precision * recall) / (0.5 * precision + recall)\n        else:\n            gleu = 0\n        return gleu\n\n    # Initialize the GLEU scores\n    gleu_scores = []\n\n    # Loop over all reference-prediction pairs\n    for ref, hyp in zip(references, predictions):\n        ref_tokens = ref.split()  # Tokenize the reference sentence\n        hyp_tokens = hyp.split()  # Tokenize the hypothesis sentence\n\n        gleu_score = compute_sentence_gleu(ref_tokens, hyp_tokens)\n        gleu_scores.append(gleu_score)\n\n    # Average GLEU score across all sentences\n    avg_gleu = sum(gleu_scores) / len(gleu_scores) if gleu_scores else 0\n    return avg_gleu\n\ngleu_score = calculate_gleu(references, predictions)\nprint(f\"GLEU Score: {gleu_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:04:54.494115Z","iopub.execute_input":"2025-03-13T13:04:54.494470Z","iopub.status.idle":"2025-03-13T13:04:54.619751Z","shell.execute_reply.started":"2025-03-13T13:04:54.494446Z","shell.execute_reply":"2025-03-13T13:04:54.618629Z"}},"outputs":[{"name":"stdout","text":"GLEU Score: 0.5896\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import torch\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu\n\n# Ensure nltk package is ready\nnltk.download('punkt')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define Tamil samples with similar errors\ntamil_samples = [\n    # 5-word sentences (5 samples)\n    {\"incorrect\": \"வாக்காளர் வழங்கப்பட்டது அரசு அறிவிப்பு\", \"corrected\": \"வாக்காளர்களுக்கு வழங்கப்பட்டது அரசு அறிவிப்பு\"},\n    {\"incorrect\": \"அறிவியல் தொழில்நுட்பம் வளர்ச்சி முக்கியம்\", \"corrected\": \"அறிவியல் மற்றும் தொழில்நுட்ப வளர்ச்சி முக்கியம்\"},\n    {\"incorrect\": \"அன்பு பாராட்டு செய்தார் மக்கள் முன்னிலையில்\", \"corrected\": \"அன்புப் பாராட்டு செய்தார் மக்கள் முன்னிலையில்\"},\n    {\"incorrect\": \"அவசர நடவடிக்க எடுக்க வேண்டும்\", \"corrected\": \"அவசர நடவடிக்கை எடுக்க வேண்டும்\"},\n    {\"incorrect\": \"நடிகர் பேசினார் ரசிகர்கள் கூட்டத்தில்\", \"corrected\": \"நடிகர் பேசினார் ரசிகர்கள் கூட்டத்தில்\"},\n\n    # 4-word sentences (8 samples)\n    {\"incorrect\": \"மழை காரணம் வெள்ளம் ஏற்பட்டது\", \"corrected\": \"மழை காரணமாக வெள்ளம் ஏற்பட்டது\"},\n    {\"incorrect\": \"நாடு முன்னேற்ற வளர்ச்சி இன்றியமையாது\", \"corrected\": \"நாடு முன்னேற்றம் வளர்ச்சி இன்றியமையாது\"},\n    {\"incorrect\": \"சுற்றுச்சூழல் பாதுகாப்பு மிக முக்கியம்\", \"corrected\": \"சுற்றுச்சூழல் பாதுகாப்பு மிக முக்கியம்\"},\n    {\"incorrect\": \"அவன் விரைவில் சென்று விட்டான்\", \"corrected\": \"அவன் விரைவாக சென்று விட்டான்\"},\n    {\"incorrect\": \"படிப்பு முடித்த வேலை பெற்றான்\", \"corrected\": \"படிப்பு முடித்து வேலை பெற்றான்\"},\n    {\"incorrect\": \"நேரம் செலவழிக்க மிக முக்கியம்\", \"corrected\": \"நேரம் செலவழிக்க மிக முக்கியம்\"},\n    {\"incorrect\": \"மகிழ்ச்சி உணர்வு மனதை உற்சாகம்\", \"corrected\": \"மகிழ்ச்சி உணர்வு மனதை உற்சாகம்\"},\n    {\"incorrect\": \"மழை காலநிலை கடுமையாக உள்ளது\", \"corrected\": \"மழைக்கால நிலை கடுமையாக உள்ளது\"},\n\n    # 3-word sentences (2 samples)\n    {\"incorrect\": \"நான் பெற்ற மதிப்பெண்\", \"corrected\": \"நான் பெற்ற மதிப்பெண்கள்\"},\n    {\"incorrect\": \"சிறந்த இசை பாடல்\", \"corrected\": \"சிறந்த இசைப் பாடல்\"},\n]\n\ntotal_bleu_score = 0.0  # Store cumulative BLEU score\nnum_samples = len(tamil_samples)\n\nfor sample in tamil_samples:\n    incorrect_sentence = sample[\"incorrect\"]\n    corrected_sentence = sample[\"corrected\"]\n\n    # Tokenize the incorrect sentence\n    inputs = tokenizer(incorrect_sentence, return_tensors=\"pt\")\n\n    # Move inputs to the same device as the model (GPU if available)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n\n    # Generate the predicted sentence\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            max_length=50,\n            num_beams=5,  \n            temperature=0.7,  \n            top_k=50, \n            top_p=0.9,\n            early_stopping=True\n        )\n\n    # Decode the predicted sentence\n    predicted_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Compute BLEU score\n    reference = [nltk.word_tokenize(corrected_sentence)]\n    candidate = nltk.word_tokenize(predicted_sentence)\n\n    bleu_score = sentence_bleu(reference, candidate, weights=(0.5, 0.5))  # Bi-gram BLEU score\n    total_bleu_score += bleu_score\n\n    # Print results\n    print(\"\\nIncorrect Sentence:\", incorrect_sentence)\n    print(\"Corrected Sentence:\", corrected_sentence)\n    print(\"Predicted Sentence:\", predicted_sentence)\n    print(\"BLEU Score: {:.2f}\".format(bleu_score))\n\n# Compute the average BLEU score\naverage_bleu_score = total_bleu_score / num_samples\nprint(\"\\nAverage BLEU Score: {:.2f}\".format(average_bleu_score))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:04:54.620730Z","iopub.execute_input":"2025-03-13T13:04:54.621005Z","iopub.status.idle":"2025-03-13T13:04:59.709778Z","shell.execute_reply.started":"2025-03-13T13:04:54.620983Z","shell.execute_reply":"2025-03-13T13:04:59.708860Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nIncorrect Sentence: வாக்காளர் வழங்கப்பட்டது அரசு அறிவிப்பு\nCorrected Sentence: வாக்காளர்களுக்கு வழங்கப்பட்டது அரசு அறிவிப்பு\nPredicted Sentence: வாக்காளர் வழங்கப்பட்டது அரசு அறிவிப்பு\nBLEU Score: 0.71\n\nIncorrect Sentence: அறிவியல் தொழில்நுட்பம் வளர்ச்சி முக்கியம்\nCorrected Sentence: அறிவியல் மற்றும் தொழில்நுட்ப வளர்ச்சி முக்கியம்\nPredicted Sentence: அறிவியல் தொழில்நுட்பம் வளர்ச்சி முக்கியம்\nBLEU Score: 0.39\n\nIncorrect Sentence: அன்பு பாராட்டு செய்தார் மக்கள் முன்னிலையில்\nCorrected Sentence: அன்புப் பாராட்டு செய்தார் மக்கள் முன்னிலையில்\nPredicted Sentence: அன்பு பாராட்டு செய்தார் மக்கள் முன்னிலையில்\nBLEU Score: 0.77\n\nIncorrect Sentence: அவசர நடவடிக்க எடுக்க வேண்டும்\nCorrected Sentence: அவசர நடவடிக்கை எடுக்க வேண்டும்\nPredicted Sentence: அவசர நடவடிக்கை எடுக்க வேண்டும்\nBLEU Score: 1.00\n\nIncorrect Sentence: நடிகர் பேசினார் ரசிகர்கள் கூட்டத்தில்\nCorrected Sentence: நடிகர் பேசினார் ரசிகர்கள் கூட்டத்தில்\nPredicted Sentence: நடிகர் பேசினார் ரசிகர்கள் கூட்டத்தில்\nBLEU Score: 1.00\n\nIncorrect Sentence: மழை காரணம் வெள்ளம் ஏற்பட்டது\nCorrected Sentence: மழை காரணமாக வெள்ளம் ஏற்பட்டது\nPredicted Sentence: மழைக் காரணம் வெள்ளம் ஏற்பட்டது\nBLEU Score: 0.41\n\nIncorrect Sentence: நாடு முன்னேற்ற வளர்ச்சி இன்றியமையாது\nCorrected Sentence: நாடு முன்னேற்றம் வளர்ச்சி இன்றியமையாது\nPredicted Sentence: நாடு முன்னேற்றத் வளர்ச்சி இன்றியமையாது\nBLEU Score: 0.50\n\nIncorrect Sentence: சுற்றுச்சூழல் பாதுகாப்பு மிக முக்கியம்\nCorrected Sentence: சுற்றுச்சூழல் பாதுகாப்பு மிக முக்கியம்\nPredicted Sentence: சுற்றுச்சூழல் பாதுகாப்பு மிக முக்கியம்\nBLEU Score: 1.00\n\nIncorrect Sentence: அவன் விரைவில் சென்று விட்டான்\nCorrected Sentence: அவன் விரைவாக சென்று விட்டான்\nPredicted Sentence: அவர் விரைவில் சென்று விட்டான்\nBLEU Score: 0.41\n\nIncorrect Sentence: படிப்பு முடித்த வேலை பெற்றான்\nCorrected Sentence: படிப்பு முடித்து வேலை பெற்றான்\nPredicted Sentence: படிப்பு முடித்த வேலை பெற்றான்\nBLEU Score: 0.50\n\nIncorrect Sentence: நேரம் செலவழிக்க மிக முக்கியம்\nCorrected Sentence: நேரம் செலவழிக்க மிக முக்கியம்\nPredicted Sentence: நேரம் செலவழிக்க மிக முக்கியம்\nBLEU Score: 1.00\n\nIncorrect Sentence: மகிழ்ச்சி உணர்வு மனதை உற்சாகம்\nCorrected Sentence: மகிழ்ச்சி உணர்வு மனதை உற்சாகம்\nPredicted Sentence: மகிழ்ச்சி உணர்வு மனதை உற்சாகம்\nBLEU Score: 1.00\n\nIncorrect Sentence: மழை காலநிலை கடுமையாக உள்ளது\nCorrected Sentence: மழைக்கால நிலை கடுமையாக உள்ளது\nPredicted Sentence: மழைக்காலநிலைக் கடுமையாக உள்ளது\nBLEU Score: 0.41\n\nIncorrect Sentence: நான் பெற்ற மதிப்பெண்\nCorrected Sentence: நான் பெற்ற மதிப்பெண்கள்\nPredicted Sentence: நான் பெற்ற மதிப்பெண்\nBLEU Score: 0.58\n\nIncorrect Sentence: சிறந்த இசை பாடல்\nCorrected Sentence: சிறந்த இசைப் பாடல்\nPredicted Sentence: சிறந்த இசைப் பாடல்\nBLEU Score: 1.00\n\nAverage BLEU Score: 0.71\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:09:44.517639Z","iopub.execute_input":"2025-03-13T13:09:44.518007Z","iopub.status.idle":"2025-03-13T13:09:49.344015Z","shell.execute_reply.started":"2025-03-13T13:09:44.517979Z","shell.execute_reply":"2025-03-13T13:09:49.342903Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.1.1 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import torch\nimport nltk\nimport sacrebleu\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.gleu_score import sentence_gleu  # Import for GLEU\n\n# Ensure nltk package is ready\nnltk.download('punkt')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define Tamil samples with similar errors\ntamil_samples = [\n    # 5-word sentences (5 samples)\n    {\"incorrect\": \"வாக்காளர் வழங்கப்பட்டது அரசு அறிவிப்பு\", \"corrected\": \"வாக்காளர்களுக்கு வழங்கப்பட்டது அரசு அறிவிப்பு\"},\n    {\"incorrect\": \"அறிவியல் தொழில்நுட்பம் வளர்ச்சி முக்கியம்\", \"corrected\": \"அறிவியல் மற்றும் தொழில்நுட்ப வளர்ச்சி முக்கியம்\"},\n    {\"incorrect\": \"அன்பு பாராட்டு செய்தார் மக்கள் முன்னிலையில்\", \"corrected\": \"அன்புப் பாராட்டு செய்தார் மக்கள் முன்னிலையில்\"},\n    {\"incorrect\": \"அவசர நடவடிக்க எடுக்க வேண்டும்\", \"corrected\": \"அவசர நடவடிக்கை எடுக்க வேண்டும்\"},\n    {\"incorrect\": \"நடிகர் பேசினார் ரசிகர்கள் கூட்டத்தில்\", \"corrected\": \"நடிகர் பேசினார் ரசிகர்கள் கூட்டத்தில்\"},\n\n    # 4-word sentences (8 samples)\n    {\"incorrect\": \"மழை காரணம் வெள்ளம் ஏற்பட்டது\", \"corrected\": \"மழை காரணமாக வெள்ளம் ஏற்பட்டது\"},\n    {\"incorrect\": \"நாடு முன்னேற்ற வளர்ச்சி இன்றியமையாது\", \"corrected\": \"நாடு முன்னேற்றம் வளர்ச்சி இன்றியமையாது\"},\n    {\"incorrect\": \"சுற்றுச்சூழல் பாதுகாப்பு மிக முக்கியம்\", \"corrected\": \"சுற்றுச்சூழல் பாதுகாப்பு மிக முக்கியம்\"},\n    {\"incorrect\": \"அவன் விரைவில் சென்று விட்டான்\", \"corrected\": \"அவன் விரைவாக சென்று விட்டான்\"},\n    {\"incorrect\": \"படிப்பு முடித்த வேலை பெற்றான்\", \"corrected\": \"படிப்பு முடித்து வேலை பெற்றான்\"},\n    {\"incorrect\": \"நேரம் செலவழிக்க மிக முக்கியம்\", \"corrected\": \"நேரம் செலவழிக்க மிக முக்கியம்\"},\n    {\"incorrect\": \"மகிழ்ச்சி உணர்வு மனதை உற்சாகம்\", \"corrected\": \"மகிழ்ச்சி உணர்வு மனதை உற்சாகம்\"},\n    {\"incorrect\": \"மழை காலநிலை கடுமையாக உள்ளது\", \"corrected\": \"மழைக்கால நிலை கடுமையாக உள்ளது\"},\n\n    # 3-word sentences (2 samples)\n    {\"incorrect\": \"நான் பெற்ற மதிப்பெண்\", \"corrected\": \"நான் பெற்ற மதிப்பெண்கள்\"},\n    {\"incorrect\": \"சிறந்த இசை பாடல்\", \"corrected\": \"சிறந்த இசைப் பாடல்\"},\n]\n\ntotal_bleu_score = 0.0\ntotal_gleu_score = 0.0\nreferences = []  # Store references for TER\nhypotheses = []  # Store hypotheses for TER\n\nnum_samples = len(tamil_samples)\n\nfor sample in tamil_samples:\n    incorrect_sentence = sample[\"incorrect\"]\n    corrected_sentence = sample[\"corrected\"]\n\n    # Tokenize the incorrect sentence\n    inputs = tokenizer(incorrect_sentence, return_tensors=\"pt\")\n\n    # Move inputs to the same device as the model (GPU if available)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n\n    # Generate the predicted sentence\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            max_length=50,\n            num_beams=5,  \n            temperature=0.7,  \n            top_k=50, \n            top_p=0.9,\n            early_stopping=True\n        )\n\n    # Decode the predicted sentence\n    predicted_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Compute BLEU score\n    reference = [nltk.word_tokenize(corrected_sentence)]\n    candidate = nltk.word_tokenize(predicted_sentence)\n    bleu_score = sentence_bleu(reference, candidate, weights=(0.5, 0.5))  # Bi-gram BLEU score\n    total_bleu_score += bleu_score\n\n    # Compute GLEU score (Google BLEU)\n    gleu_score = sentence_gleu(reference, candidate)\n    total_gleu_score += gleu_score\n\n    # Store sentences for TER calculation\n    references.append([corrected_sentence])\n    hypotheses.append(predicted_sentence)\n\n    # Print results\n    print(\"\\nIncorrect Sentence:\", incorrect_sentence)\n    print(\"Corrected Sentence:\", corrected_sentence)\n    print(\"Predicted Sentence:\", predicted_sentence)\n    print(\"BLEU Score: {:.2f}\".format(bleu_score))\n    print(\"GLEU Score: {:.2f}\".format(gleu_score))\n\n# Compute the average BLEU and GLEU scores\naverage_bleu_score = total_bleu_score / num_samples\naverage_gleu_score = total_gleu_score / num_samples\n\n# Compute TER score\nter_score = sacrebleu.corpus_ter(hypotheses, references).score\n\nprint(\"\\nAverage BLEU Score: {:.2f}\".format(average_bleu_score))\nprint(\"Average GLEU Score: {:.2f}\".format(average_gleu_score))\nprint(\"TER Score: {:.2f}\".format(ter_score))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:10:22.752807Z","iopub.execute_input":"2025-03-13T13:10:22.753523Z","iopub.status.idle":"2025-03-13T13:10:26.879880Z","shell.execute_reply.started":"2025-03-13T13:10:22.753496Z","shell.execute_reply":"2025-03-13T13:10:26.879058Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\nIncorrect Sentence: வாக்காளர் வழங்கப்பட்டது அரசு அறிவிப்பு\nCorrected Sentence: வாக்காளர்களுக்கு வழங்கப்பட்டது அரசு அறிவிப்பு\nPredicted Sentence: வாக்காளர் வழங்கப்பட்டது அரசு அறிவிப்பு\nBLEU Score: 0.71\nGLEU Score: 0.60\n\nIncorrect Sentence: அறிவியல் தொழில்நுட்பம் வளர்ச்சி முக்கியம்\nCorrected Sentence: அறிவியல் மற்றும் தொழில்நுட்ப வளர்ச்சி முக்கியம்\nPredicted Sentence: அறிவியல் தொழில்நுட்பம் வளர்ச்சி முக்கியம்\nBLEU Score: 0.39\nGLEU Score: 0.29\n\nIncorrect Sentence: அன்பு பாராட்டு செய்தார் மக்கள் முன்னிலையில்\nCorrected Sentence: அன்புப் பாராட்டு செய்தார் மக்கள் முன்னிலையில்\nPredicted Sentence: அன்பு பாராட்டு செய்தார் மக்கள் முன்னிலையில்\nBLEU Score: 0.77\nGLEU Score: 0.71\n\nIncorrect Sentence: அவசர நடவடிக்க எடுக்க வேண்டும்\nCorrected Sentence: அவசர நடவடிக்கை எடுக்க வேண்டும்\nPredicted Sentence: அவசர நடவடிக்கை எடுக்க வேண்டும்\nBLEU Score: 1.00\nGLEU Score: 1.00\n\nIncorrect Sentence: நடிகர் பேசினார் ரசிகர்கள் கூட்டத்தில்\nCorrected Sentence: நடிகர் பேசினார் ரசிகர்கள் கூட்டத்தில்\nPredicted Sentence: நடிகர் பேசினார் ரசிகர்கள் கூட்டத்தில்\nBLEU Score: 1.00\nGLEU Score: 1.00\n\nIncorrect Sentence: மழை காரணம் வெள்ளம் ஏற்பட்டது\nCorrected Sentence: மழை காரணமாக வெள்ளம் ஏற்பட்டது\nPredicted Sentence: மழைக் காரணம் வெள்ளம் ஏற்பட்டது\nBLEU Score: 0.41\nGLEU Score: 0.30\n\nIncorrect Sentence: நாடு முன்னேற்ற வளர்ச்சி இன்றியமையாது\nCorrected Sentence: நாடு முன்னேற்றம் வளர்ச்சி இன்றியமையாது\nPredicted Sentence: நாடு முன்னேற்றத் வளர்ச்சி இன்றியமையாது\nBLEU Score: 0.50\nGLEU Score: 0.40\n\nIncorrect Sentence: சுற்றுச்சூழல் பாதுகாப்பு மிக முக்கியம்\nCorrected Sentence: சுற்றுச்சூழல் பாதுகாப்பு மிக முக்கியம்\nPredicted Sentence: சுற்றுச்சூழல் பாதுகாப்பு மிக முக்கியம்\nBLEU Score: 1.00\nGLEU Score: 1.00\n\nIncorrect Sentence: அவன் விரைவில் சென்று விட்டான்\nCorrected Sentence: அவன் விரைவாக சென்று விட்டான்\nPredicted Sentence: அவர் விரைவில் சென்று விட்டான்\nBLEU Score: 0.41\nGLEU Score: 0.30\n\nIncorrect Sentence: படிப்பு முடித்த வேலை பெற்றான்\nCorrected Sentence: படிப்பு முடித்து வேலை பெற்றான்\nPredicted Sentence: படிப்பு முடித்த வேலை பெற்றான்\nBLEU Score: 0.50\nGLEU Score: 0.40\n\nIncorrect Sentence: நேரம் செலவழிக்க மிக முக்கியம்\nCorrected Sentence: நேரம் செலவழிக்க மிக முக்கியம்\nPredicted Sentence: நேரம் செலவழிக்க மிக முக்கியம்\nBLEU Score: 1.00\nGLEU Score: 1.00\n\nIncorrect Sentence: மகிழ்ச்சி உணர்வு மனதை உற்சாகம்\nCorrected Sentence: மகிழ்ச்சி உணர்வு மனதை உற்சாகம்\nPredicted Sentence: மகிழ்ச்சி உணர்வு மனதை உற்சாகம்\nBLEU Score: 1.00\nGLEU Score: 1.00\n\nIncorrect Sentence: மழை காலநிலை கடுமையாக உள்ளது\nCorrected Sentence: மழைக்கால நிலை கடுமையாக உள்ளது\nPredicted Sentence: மழைக்காலநிலைக் கடுமையாக உள்ளது\nBLEU Score: 0.41\nGLEU Score: 0.30\n\nIncorrect Sentence: நான் பெற்ற மதிப்பெண்\nCorrected Sentence: நான் பெற்ற மதிப்பெண்கள்\nPredicted Sentence: நான் பெற்ற மதிப்பெண்\nBLEU Score: 0.58\nGLEU Score: 0.50\n\nIncorrect Sentence: சிறந்த இசை பாடல்\nCorrected Sentence: சிறந்த இசைப் பாடல்\nPredicted Sentence: சிறந்த இசைப் பாடல்\nBLEU Score: 1.00\nGLEU Score: 1.00\n\nAverage BLEU Score: 0.71\nAverage GLEU Score: 0.65\nTER Score: 25.00\n","output_type":"stream"}],"execution_count":20}]}