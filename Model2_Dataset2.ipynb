{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11020543,"sourceType":"datasetVersion","datasetId":6862353}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Load your dataset\nfile_path = \"/kaggle/input/tamil-grammar-dataset/tamil_grammar_large_dataset.csv\"  # Update with your actual file path\ndf = pd.read_csv(file_path)\n\n# Check the first few rows to understand the structure\nprint(df.head())\n\n# Remove rows with any null values in the dataset\ndf_cleaned = df.dropna()\n\n# Remove duplicate rows\ndf_cleaned = df_cleaned.drop_duplicates()\n\n# Optionally, you can reset the index after cleaning\ndf_cleaned = df_cleaned.reset_index(drop=True)\n\n# Check the cleaned dataset\nprint(df_cleaned.head())\n\n# Save the cleaned dataset to a new CSV file\ndf_cleaned.to_csv(\"cleaned_dataset.csv\", index=False)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-03-13T14:31:59.954580Z","iopub.execute_input":"2025-03-13T14:31:59.954820Z","iopub.status.idle":"2025-03-13T14:32:00.327484Z","shell.execute_reply.started":"2025-03-13T14:31:59.954799Z","shell.execute_reply":"2025-03-13T14:32:00.326790Z"},"trusted":true},"outputs":[{"name":"stdout","text":"                          Sentence              Error Type  \\\n0  ‡ÆÖ‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æö‡Ææ‡Æ™‡Øç‡Æ™‡Æø‡Æü‡ØÅ‡Æµ‡Ææ‡Æ≥‡Øç.          Spelling Error   \n1     ‡Æ®‡Ææ‡ÆÆ‡Øç ‡Æ™‡Ææ‡Æü‡Æ§‡Øç‡Æ§‡Øà ‡Æ™‡Æü‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Ææ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç.  Subject-Verb Agreement   \n2     ‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æö‡Ææ‡Æ™‡Øç‡Æ™‡Æø‡Æü‡ØÅ‡Æµ‡Ææ‡Æ≥‡Øç.          Spelling Error   \n3    ‡ÆÖ‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Æ®‡Øç‡Æ§‡ØÅ ‡Æµ‡Æø‡Æ≥‡Øà‡ÆØ‡Ææ‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Øã‡ÆÆ‡Øç.                No Error   \n4  ‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡ØÅ‡Æ§‡Øç‡Æ§‡Æï‡Æ§‡Øç‡Æ§‡Øà ‡Æµ‡Æø‡Æ≥‡Øà‡ÆØ‡Ææ‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Ææ‡Æ©‡Øç.  Subject-Verb Agreement   \n\n               Corrected Sentence  \n0  ‡ÆÖ‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æ™‡Æü‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Ææ‡Æ≥‡Øç.  \n1          ‡Æ®‡Ææ‡ÆÆ‡Øç ‡Æ™‡Ææ‡Æü‡Æ§‡Øç‡Æ§‡Øà ‡Æö‡ØÜ‡Æ≤‡Øç‡Æµ‡Øá‡Æ©‡Øç.  \n2  ‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æµ‡Æø‡Æ≥‡Øà‡ÆØ‡Ææ‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Øã‡ÆÆ‡Øç.  \n3   ‡ÆÖ‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Æ®‡Øç‡Æ§‡ØÅ ‡Æµ‡Æø‡Æ≥‡Øà‡ÆØ‡Ææ‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Øã‡ÆÆ‡Øç.  \n4    ‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡ØÅ‡Æ§‡Øç‡Æ§‡Æï‡Æ§‡Øç‡Æ§‡Øà ‡Æ™‡Æü‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Ææ‡Æ©‡Øç.  \n                          Sentence              Error Type  \\\n0  ‡ÆÖ‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æö‡Ææ‡Æ™‡Øç‡Æ™‡Æø‡Æü‡ØÅ‡Æµ‡Ææ‡Æ≥‡Øç.          Spelling Error   \n1     ‡Æ®‡Ææ‡ÆÆ‡Øç ‡Æ™‡Ææ‡Æü‡Æ§‡Øç‡Æ§‡Øà ‡Æ™‡Æü‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Ææ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç.  Subject-Verb Agreement   \n2     ‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æö‡Ææ‡Æ™‡Øç‡Æ™‡Æø‡Æü‡ØÅ‡Æµ‡Ææ‡Æ≥‡Øç.          Spelling Error   \n3    ‡ÆÖ‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Æ®‡Øç‡Æ§‡ØÅ ‡Æµ‡Æø‡Æ≥‡Øà‡ÆØ‡Ææ‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Øã‡ÆÆ‡Øç.                No Error   \n4  ‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡ØÅ‡Æ§‡Øç‡Æ§‡Æï‡Æ§‡Øç‡Æ§‡Øà ‡Æµ‡Æø‡Æ≥‡Øà‡ÆØ‡Ææ‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Ææ‡Æ©‡Øç.  Subject-Verb Agreement   \n\n               Corrected Sentence  \n0  ‡ÆÖ‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æ™‡Æü‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Ææ‡Æ≥‡Øç.  \n1          ‡Æ®‡Ææ‡ÆÆ‡Øç ‡Æ™‡Ææ‡Æü‡Æ§‡Øç‡Æ§‡Øà ‡Æö‡ØÜ‡Æ≤‡Øç‡Æµ‡Øá‡Æ©‡Øç.  \n2  ‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æµ‡Æø‡Æ≥‡Øà‡ÆØ‡Ææ‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Øã‡ÆÆ‡Øç.  \n3   ‡ÆÖ‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Æ®‡Øç‡Æ§‡ØÅ ‡Æµ‡Æø‡Æ≥‡Øà‡ÆØ‡Ææ‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Øã‡ÆÆ‡Øç.  \n4    ‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡ØÅ‡Æ§‡Øç‡Æ§‡Æï‡Æ§‡Øç‡Æ§‡Øà ‡Æ™‡Æü‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Ææ‡Æ©‡Øç.  \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_MODE\"] = \"disabled\"","metadata":{"execution":{"iopub.status.busy":"2025-03-13T14:32:06.712941Z","iopub.execute_input":"2025-03-13T14:32:06.713270Z","iopub.status.idle":"2025-03-13T14:32:06.717525Z","shell.execute_reply.started":"2025-03-13T14:32:06.713243Z","shell.execute_reply":"2025-03-13T14:32:06.716472Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,AutoModel\nimport pandas as pd\n\n# Load Tamil-specific model and tokenizer\nmodel_name = \"facebook/nllb-200-distilled-600M\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\n# Load your cleaned dataset\ndf_cleaned = pd.read_csv(\"cleaned_dataset.csv\")\n\n# Calculate max input and target lengths based on tokenized sentences\nmax_input_length = max([len(tokenizer.encode(sentence, truncation=True, padding=False)) for sentence in df_cleaned['Sentence']])\nmax_target_length = max([len(tokenizer.encode(sentence, truncation=True, padding=False)) for sentence in df_cleaned['Corrected Sentence']])\n\nprint(f\"Max Input Length: {max_input_length}\")\nprint(f\"Max Target Length: {max_target_length}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:32:10.517832Z","iopub.execute_input":"2025-03-13T14:32:10.518107Z","iopub.status.idle":"2025-03-13T14:32:46.487862Z","shell.execute_reply.started":"2025-03-13T14:32:10.518088Z","shell.execute_reply":"2025-03-13T14:32:46.486563Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"208a8a9f17a449bca207d5c709400a2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8ae1714076b496a81594554c395b6a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122e4d757ae346c4acf9c49fdf4310a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2ca836d751040a4911415c51c5d2d86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06f9b3d6c1d9431bb80406a3a6d0f344"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0adfe5f9685b4b0e8042ef88923f7efa"}},"metadata":{}},{"name":"stdout","text":"Max Input Length: 11\nMax Target Length: 10\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\n\n# Load the cleaned dataset\ndf_cleaned = pd.read_csv(\"cleaned_dataset.csv\")\n\n# Split the dataset into training and validation sets\ntrain_df, val_df = train_test_split(df_cleaned, test_size=0.1)\n\n# Convert the dataframe to Hugging Face dataset format\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Load NLLB-200 model and tokenizer\nmodel_name = \"facebook/nllb-200-distilled-600M\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Set the language code for Tamil\ntamil_lang_code = \"tam_Taml\"\n\n# Tokenizer function\ndef preprocess_function(examples):\n    inputs = examples['Sentence']\n    targets = examples['Corrected Sentence']\n\n    # Add language token for Tamil\n    inputs = [f\"{tamil_lang_code} {text}\" for text in inputs]\n    \n    # Tokenize input and target sequences\n    model_inputs = tokenizer(inputs, max_length=11, truncation=True, padding=\"max_length\")\n    labels = tokenizer(targets, max_length=10, truncation=True, padding=\"max_length\")\n\n    # Add the labels to model inputs\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    return model_inputs\n\n# Apply tokenization\ntrain_dataset = train_dataset.map(preprocess_function, batched=True)\nval_dataset = val_dataset.map(preprocess_function, batched=True)\n\n# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    warmup_steps=int(0.1 * len(train_dataset)),\n    lr_scheduler_type=\"linear\",\n    gradient_accumulation_steps=2,\n    max_grad_norm=1.0,\n    run_name=\"tamil-error-correction-nllb200\",\n    report_to=[]\n)\n\n# Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer\n)\n\n# Train the model\ntrainer.train()\n\n# Save the trained model\ntrainer.save_model(\"trained_model_nllb200\")\n\n# Evaluate the model\nresults = trainer.evaluate(val_dataset)\nprint(results)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-13T14:32:50.019149Z","iopub.execute_input":"2025-03-13T14:32:50.019833Z","iopub.status.idle":"2025-03-13T14:41:02.065107Z","shell.execute_reply.started":"2025-03-13T14:32:50.019723Z","shell.execute_reply":"2025-03-13T14:41:02.064338Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c73a590b158e4f198c38030a7738f17f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1775 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6c44aefd3394cf6a73a4c71469c901c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/198 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cf66f5dd8df40e69352f6c89974bfef"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-4-ba2effa9a302>:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='275' max='275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [275/275 07:46, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>2.606803</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.634597</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.188154</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.726022</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.7260220646858215, 'eval_runtime': 3.9216, 'eval_samples_per_second': 50.49, 'eval_steps_per_second': 3.315, 'epoch': 4.918918918918919}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract logs from Trainer state\nlog_history = trainer.state.log_history\n\n# Separate training and validation losses\ntrain_losses = [entry['loss'] for entry in log_history if 'loss' in entry]\neval_losses = [entry['eval_loss'] for entry in log_history if 'eval_loss' in entry]\nlearning_rates = [entry['learning_rate'] for entry in log_history if 'learning_rate' in entry]\n\n# Create x-axis for epochs\n\nepochs_eval = range(1, len(eval_losses) + 1)\n\n\n# Plot validation loss\nplt.figure(figsize=(6, 4))\nplt.plot(epochs_eval, eval_losses, label=\"Validation Loss\", marker='o', color='red')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Validation Loss over Epochs\")\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:41:06.836495Z","iopub.execute_input":"2025-03-13T14:41:06.836919Z","iopub.status.idle":"2025-03-13T14:41:07.126966Z","shell.execute_reply.started":"2025-03-13T14:41:06.836883Z","shell.execute_reply":"2025-03-13T14:41:07.126080Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom nltk.translate.bleu_score import sentence_bleu\n\n\n# Check if GPU is available and move the model to GPU if it is\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\n\n# Generate predictions on the validation set\npredictions = []\nreferences = []\n\n# Use the model to generate predictions on the validation set\nfor example in val_dataset:\n    input_ids = tokenizer(example['Sentence'], return_tensors='pt', padding=True, truncation=True, max_length=35).input_ids\n    \n    # Move input_ids to the correct device (same as the model)\n    input_ids = input_ids.to(device)\n    \n    output_ids = model.generate(input_ids)\n    decoded_prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)  # Use tokenizer's decode method\n\n    predictions.append(decoded_prediction)\n    references.append(example['Corrected Sentence'])\n\n# BLEU Score Calculation (with n-gram overlap)\ndef calculate_bleu(predictions, references):\n    bleu_scores = []\n    for pred, ref in zip(predictions, references):\n        # Tokenize sentences and calculate BLEU score\n        pred_tokens = pred.split()\n        ref_tokens = ref.split()\n        bleu_scores.append(sentence_bleu([ref_tokens], pred_tokens))  # List of references is passed to sentence_bleu\n    return sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n\n\n# Calculate BLEU\nbleu_score = calculate_bleu(predictions, references)\n\n\n# Print the results\nprint(f\"BLEU Score: {bleu_score:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2025-03-13T14:41:15.635524Z","iopub.execute_input":"2025-03-13T14:41:15.635873Z","iopub.status.idle":"2025-03-13T14:41:45.271445Z","shell.execute_reply.started":"2025-03-13T14:41:15.635844Z","shell.execute_reply":"2025-03-13T14:41:45.270472Z"},"trusted":true},"outputs":[{"name":"stdout","text":"BLEU Score: 0.8265\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install bert_score\nfrom bert_score import score\nP, R, F1 = score(predictions, references, lang=\"ta\")  # Adjust the language if necessary (e.g., \"ta\" for Tamil)\n\n# Print BERT scores\nprint(f\"BERT Precision: {P.mean():.4f}\")\nprint(f\"BERT Recall: {R.mean():.4f}\")\nprint(f\"BERT F1 Score: {F1.mean():.4f}\")","metadata":{"execution":{"iopub.status.busy":"2025-03-13T14:41:58.387957Z","iopub.execute_input":"2025-03-13T14:41:58.388296Z","iopub.status.idle":"2025-03-13T14:42:18.237094Z","shell.execute_reply.started":"2025-03-13T14:41:58.388268Z","shell.execute_reply":"2025-03-13T14:42:18.235833Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.47.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.29.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bert_score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7becd7f4c80948bb8fac5a144dc9b5f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8280fdd3baa4ef3a9c8b2cfe8b46b92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bec7b5a210094423beb85d8ed8eef346"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b7d108a336943039fec3b4d6abc2243"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abcccac566ec48fd99f77f27956c58e3"}},"metadata":{}},{"name":"stdout","text":"BERT Precision: 0.9354\nBERT Recall: 0.9108\nBERT F1 Score: 0.9226\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import nltk\n\ndef calculate_ter(references, predictions):\n    \"\"\"\n    Compute TER (Translation Edit Rate) score.\n    references: list of reference sentences (the correct sentences)\n    predictions: list of generated sentences (the sentences predicted by the model)\n    \"\"\"\n    # Initialize the TER scores\n    ter_scores = []\n    \n    # Loop over all the sentences\n    for ref, hyp in zip(references, predictions):\n        ref_tokens = ref.split()  # Tokenize the reference sentence\n        hyp_tokens = hyp.split()  # Tokenize the hypothesis sentence\n\n        # Find the minimum edit distance (substitutions, insertions, deletions)\n        edits = nltk.edit_distance(ref_tokens, hyp_tokens)\n\n        # TER is calculated as (number of edits + len(reference) - len(hypothesis)) / len(reference)\n        ter = (edits + len(ref_tokens) - len(hyp_tokens)) / len(ref_tokens)\n        ter_scores.append(ter)\n\n    # Average TER score across all sentences\n    avg_ter = sum(ter_scores) / len(ter_scores) if ter_scores else 0\n    return avg_ter\n\n\n\nter_score = calculate_ter(references, predictions)\nprint(f\"TER Score: {ter_score:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-03-13T14:42:52.273101Z","iopub.execute_input":"2025-03-13T14:42:52.273420Z","iopub.status.idle":"2025-03-13T14:42:52.281906Z","shell.execute_reply.started":"2025-03-13T14:42:52.273395Z","shell.execute_reply":"2025-03-13T14:42:52.281051Z"},"trusted":true},"outputs":[{"name":"stdout","text":"TER Score: 0.2407\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import nltk\nfrom collections import Counter\n\ndef calculate_gleu(references, predictions, max_order=4):\n    \"\"\"\n    Compute GLEU score.\n    references: list of reference sentences (the correct sentences)\n    predictions: list of generated sentences (the sentences predicted by the model)\n    max_order: maximum n-gram length for the BLEU calculation (default is 4)\n    \"\"\"\n    def get_ngrams(tokens, n):\n        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n\n    def compute_sentence_gleu(ref_tokens, hyp_tokens):\n        \"\"\"\n        Compute the GLEU score for a single sentence pair.\n        \"\"\"\n        total_ref_ngrams = Counter()\n        total_hyp_ngrams = Counter()\n        total_match_ngrams = Counter()\n\n        for n in range(1, max_order + 1):\n            ref_ngrams = get_ngrams(ref_tokens, n)\n            hyp_ngrams = get_ngrams(hyp_tokens, n)\n            \n            ref_ngrams_count = Counter(ref_ngrams)\n            hyp_ngrams_count = Counter(hyp_ngrams)\n\n            total_ref_ngrams.update(ref_ngrams_count)\n            total_hyp_ngrams.update(hyp_ngrams_count)\n            total_match_ngrams.update(ref_ngrams_count & hyp_ngrams_count)  # Intersection for matching n-grams\n\n        # Compute precision and recall\n        precision = sum(total_match_ngrams.values()) / sum(total_hyp_ngrams.values()) if total_hyp_ngrams else 0\n        recall = sum(total_match_ngrams.values()) / sum(total_ref_ngrams.values()) if total_ref_ngrams else 0\n\n        # Compute GLEU score (harmonic mean of precision and recall)\n        if precision + recall > 0:\n            gleu = (1 + 0.5) * (precision * recall) / (0.5 * precision + recall)\n        else:\n            gleu = 0\n        return gleu\n\n    # Initialize the GLEU scores\n    gleu_scores = []\n\n    # Loop over all reference-prediction pairs\n    for ref, hyp in zip(references, predictions):\n        ref_tokens = ref.split()  # Tokenize the reference sentence\n        hyp_tokens = hyp.split()  # Tokenize the hypothesis sentence\n\n        gleu_score = compute_sentence_gleu(ref_tokens, hyp_tokens)\n        gleu_scores.append(gleu_score)\n\n    # Average GLEU score across all sentences\n    avg_gleu = sum(gleu_scores) / len(gleu_scores) if gleu_scores else 0\n    return avg_gleu\n\ngleu_score = calculate_gleu(references, predictions)\nprint(f\"GLEU Score: {gleu_score:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-03-13T14:43:01.992630Z","iopub.execute_input":"2025-03-13T14:43:01.992971Z","iopub.status.idle":"2025-03-13T14:43:02.008235Z","shell.execute_reply.started":"2025-03-13T14:43:01.992943Z","shell.execute_reply":"2025-03-13T14:43:02.007325Z"},"trusted":true},"outputs":[{"name":"stdout","text":"GLEU Score: 0.6389\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import random\nimport torch\nfrom nltk.translate.bleu_score import sentence_bleu\n\n# Ensure model is on the correct device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Step 1: Select 5 random samples from val_dataset\nrandom_samples = random.sample(list(val_dataset), 5)\n\n# Step 2: Generate predictions and store results\npredictions = []\nreferences = []\n\nfor sample in random_samples:\n    incorrect_sentence = sample['Sentence']\n    corrected_sentence = sample['Corrected Sentence']\n\n    # Tokenize the incorrect sentence\n    inputs = tokenizer(incorrect_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=50)\n\n    # Move inputs to the correct device\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n\n    # Generate prediction\n    with torch.no_grad():\n        outputs = model.generate(input_ids=inputs['input_ids'], max_length=50, num_beams=4, early_stopping=True)\n\n    # Decode predicted sentence\n    predicted_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Store results\n    predictions.append(predicted_sentence)\n    references.append(corrected_sentence)\n\n    # Print each sample\n    print(\"\\n--- Sample ---\")\n    print(f\"Incorrect Sentence: {incorrect_sentence}\")\n    print(f\"Corrected Sentence: {corrected_sentence}\")\n    print(f\"Predicted Sentence: {predicted_sentence}\")\n\n# Step 3: BLEU Score Calculation\ndef calculate_bleu(predictions, references):\n    bleu_scores = []\n    for pred, ref in zip(predictions, references):\n        pred_tokens = tokenizer.tokenize(pred)  # Use model's tokenizer\n        ref_tokens = tokenizer.tokenize(ref)  # Use model's tokenizer\n        bleu_scores.append(sentence_bleu([ref_tokens], pred_tokens))  # List of references needed\n    return sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n\n# Compute BLEU score\naverage_bleu = calculate_bleu(predictions, references)\nprint(f\"\\n‚úÖ Average BLEU Score (5 samples): {average_bleu:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:43:06.483073Z","iopub.execute_input":"2025-03-13T14:43:06.483398Z","iopub.status.idle":"2025-03-13T14:43:07.558370Z","shell.execute_reply.started":"2025-03-13T14:43:06.483363Z","shell.execute_reply":"2025-03-13T14:43:07.557531Z"}},"outputs":[{"name":"stdout","text":"\n--- Sample ---\nIncorrect Sentence: ‡Æ™‡Ææ‡Æ∞‡Øç‡Æµ‡Æ§‡Æø ‡Æµ‡ØÄ‡Æü‡Øç‡Æü‡Æø‡Æ≤‡Øç ‡Æö‡Ææ‡Æ™‡Øç‡Æ™‡Æø‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Ææ‡Æ≥‡Øç.\nCorrected Sentence: ‡Æ™‡Ææ‡Æ∞‡Øç‡Æµ‡Æ§‡Æø ‡Æµ‡ØÄ‡Æü‡Øç‡Æü‡Æø‡Æ≤‡Øç ‡Æö‡ØÜ‡Æ≤‡Øç‡Æµ‡Øá‡Æ©‡Øç.\nPredicted Sentence: ‡Æ™‡Ææ‡Æ∞‡Øç‡Æµ‡Æ§‡Æø ‡Æµ‡ØÄ‡Æü‡Øç‡Æü‡Æø‡Æ≤‡Øç ‡Æ™‡Æü‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Ææ‡Æ©‡Øç.\n\n--- Sample ---\nIncorrect Sentence: ‡ÆÖ‡Æµ‡Æ≥‡Øç ‡Æµ‡ØÄ‡Æü‡Øç‡Æü‡Æø‡Æ≤‡Øç ‡Æö‡Ææ‡Æ™‡Øç‡Æ™‡Æø‡Æü‡ØÅ‡Æµ‡Ææ‡Æ≥‡Øç.\nCorrected Sentence: ‡ÆÖ‡Æµ‡Æ≥‡Øç ‡Æµ‡ØÄ‡Æü‡Øç‡Æü‡Æø‡Æ≤‡Øç ‡Æö‡ØÜ‡Æ≤‡Øç‡Æµ‡Øá‡Æ©‡Øç.\nPredicted Sentence: ‡ÆÖ‡Æµ‡Æ≥‡Øç ‡Æµ‡ØÄ‡Æü‡Øç‡Æü‡Æø‡Æ≤‡Øç ‡Æ™‡Æü‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Ææ‡Æ≥‡Øç.\n\n--- Sample ---\nIncorrect Sentence: ‡Æ®‡Ææ‡ÆÆ‡Øç ‡Æ™‡ØÅ‡Æ§‡Øç‡Æ§‡Æï‡Æ§‡Øç‡Æ§‡Øà ‡Æö‡ØÜ‡Æ≤‡Øç‡Æµ‡Øá‡Æ©‡Øç.\nCorrected Sentence: ‡Æ®‡Ææ‡ÆÆ‡Øç ‡Æ™‡ØÅ‡Æ§‡Øç‡Æ§‡Æï‡Æ§‡Øç‡Æ§‡Øà ‡Æö‡ØÜ‡Æ≤‡Øç‡Æµ‡Øá‡Æ©‡Øç.\nPredicted Sentence: ‡Æ®‡Ææ‡ÆÆ‡Øç ‡Æ™‡ØÅ‡Æ§‡Øç‡Æ§‡Æï‡Æ§‡Øç‡Æ§‡Øà ‡Æö‡ØÜ‡Æ≤‡Øç‡Æµ‡Øá‡Æ©‡Øç.\n\n--- Sample ---\nIncorrect Sentence: ‡ÆÖ‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Ææ‡Æü‡Æ§‡Øç‡Æ§‡Øà ‡Æö‡Ææ‡Æ™‡Øç‡Æ™‡Æø‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Ææ‡Æ≥‡Øç.\nCorrected Sentence: ‡ÆÖ‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Ææ‡Æü‡Æ§‡Øç‡Æ§‡Øà ‡Æ™‡Æü‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Ææ‡Æ©‡Øç.\nPredicted Sentence: ‡ÆÖ‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Ææ‡Æü‡Æ§‡Øç‡Æ§‡Øà ‡Æö‡Ææ‡Æ™‡Øç‡Æ™‡Æø‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Ææ‡Æ©‡Øç.\n\n--- Sample ---\nIncorrect Sentence: ‡Æ®‡Ææ‡Æ©‡Øç ‡Æµ‡ØÄ‡Æü‡Øç‡Æü‡Æø‡Æ≤‡Øç ‡Æö‡Ææ‡Æ™‡Øç‡Æ™‡Æø‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Ææ‡Æ≥‡Øç.\nCorrected Sentence: ‡Æ®‡Ææ‡Æ©‡Øç ‡Æµ‡ØÄ‡Æü‡Øç‡Æü‡Æø‡Æ≤‡Øç ‡Æ™‡Æü‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Ææ‡Æ≥‡Øç.\nPredicted Sentence: ‡Æ®‡Ææ‡Æ©‡Øç ‡Æµ‡ØÄ‡Æü‡Øç‡Æü‡Æø‡Æ≤‡Øç ‡Æ™‡Æü‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Ææ‡Æ≥‡Øç.\n\n‚úÖ Average BLEU Score (5 samples): 0.6892\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:46:45.824158Z","iopub.execute_input":"2025-03-13T14:46:45.824524Z","iopub.status.idle":"2025-03-13T14:46:49.593054Z","shell.execute_reply.started":"2025-03-13T14:46:45.824495Z","shell.execute_reply":"2025-03-13T14:46:49.592173Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.1.1 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch\nimport nltk\nimport sacrebleu\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.gleu_score import sentence_gleu  # Import for GLEU\n\n# Ensure nltk package is ready\nnltk.download('punkt')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define Tamil samples with similar errors\ntamil_samples = [\n    # 5-word sentences (5 samples)\n    {\"incorrect\": \"‡Æµ‡Ææ‡Æï‡Øç‡Æï‡Ææ‡Æ≥‡Æ∞‡Øç ‡Æµ‡Æ¥‡Æô‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ§‡ØÅ ‡ÆÖ‡Æ∞‡Æö‡ØÅ ‡ÆÖ‡Æ±‡Æø‡Æµ‡Æø‡Æ™‡Øç‡Æ™‡ØÅ\", \"corrected\": \"‡Æµ‡Ææ‡Æï‡Øç‡Æï‡Ææ‡Æ≥‡Æ∞‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æµ‡Æ¥‡Æô‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ§‡ØÅ ‡ÆÖ‡Æ∞‡Æö‡ØÅ ‡ÆÖ‡Æ±‡Æø‡Æµ‡Æø‡Æ™‡Øç‡Æ™‡ØÅ\"},\n    {\"incorrect\": \"‡ÆÖ‡Æ±‡Æø‡Æµ‡Æø‡ÆØ‡Æ≤‡Øç ‡Æ§‡Øä‡Æ¥‡Æø‡Æ≤‡Øç‡Æ®‡ØÅ‡Æü‡Øç‡Æ™‡ÆÆ‡Øç ‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç\", \"corrected\": \"‡ÆÖ‡Æ±‡Æø‡Æµ‡Æø‡ÆØ‡Æ≤‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æ§‡Øä‡Æ¥‡Æø‡Æ≤‡Øç‡Æ®‡ØÅ‡Æü‡Øç‡Æ™ ‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç\"},\n    {\"incorrect\": \"‡ÆÖ‡Æ©‡Øç‡Æ™‡ØÅ ‡Æ™‡Ææ‡Æ∞‡Ææ‡Æü‡Øç‡Æü‡ØÅ ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Ææ‡Æ∞‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Æø‡Æ≤‡Øà‡ÆØ‡Æø‡Æ≤‡Øç\", \"corrected\": \"‡ÆÖ‡Æ©‡Øç‡Æ™‡ØÅ‡Æ™‡Øç ‡Æ™‡Ææ‡Æ∞‡Ææ‡Æü‡Øç‡Æü‡ØÅ ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Ææ‡Æ∞‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Æø‡Æ≤‡Øà‡ÆØ‡Æø‡Æ≤‡Øç\"},\n    {\"incorrect\": \"‡ÆÖ‡Æµ‡Æö‡Æ∞ ‡Æ®‡Æü‡Æµ‡Æü‡Æø‡Æï‡Øç‡Æï ‡Æé‡Æü‡ØÅ‡Æï‡Øç‡Æï ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç\", \"corrected\": \"‡ÆÖ‡Æµ‡Æö‡Æ∞ ‡Æ®‡Æü‡Æµ‡Æü‡Æø‡Æï‡Øç‡Æï‡Øà ‡Æé‡Æü‡ØÅ‡Æï‡Øç‡Æï ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç\"},\n    {\"incorrect\": \"‡Æ®‡Æü‡Æø‡Æï‡Æ∞‡Øç ‡Æ™‡Øá‡Æö‡Æø‡Æ©‡Ææ‡Æ∞‡Øç ‡Æ∞‡Æö‡Æø‡Æï‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡ØÇ‡Æü‡Øç‡Æü‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç\", \"corrected\": \"‡Æ®‡Æü‡Æø‡Æï‡Æ∞‡Øç ‡Æ™‡Øá‡Æö‡Æø‡Æ©‡Ææ‡Æ∞‡Øç ‡Æ∞‡Æö‡Æø‡Æï‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡ØÇ‡Æü‡Øç‡Æü‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç\"},\n\n    # 4-word sentences (8 samples)\n    {\"incorrect\": \"‡ÆÆ‡Æ¥‡Øà ‡Æï‡Ææ‡Æ∞‡Æ£‡ÆÆ‡Øç ‡Æµ‡ØÜ‡Æ≥‡Øç‡Æ≥‡ÆÆ‡Øç ‡Æè‡Æ±‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ§‡ØÅ\", \"corrected\": \"‡ÆÆ‡Æ¥‡Øà ‡Æï‡Ææ‡Æ∞‡Æ£‡ÆÆ‡Ææ‡Æï ‡Æµ‡ØÜ‡Æ≥‡Øç‡Æ≥‡ÆÆ‡Øç ‡Æè‡Æ±‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ§‡ØÅ\"},\n    {\"incorrect\": \"‡Æ®‡Ææ‡Æü‡ØÅ ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Øá‡Æ±‡Øç‡Æ± ‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æá‡Æ©‡Øç‡Æ±‡Æø‡ÆØ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ§‡ØÅ\", \"corrected\": \"‡Æ®‡Ææ‡Æü‡ØÅ ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Øá‡Æ±‡Øç‡Æ±‡ÆÆ‡Øç ‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æá‡Æ©‡Øç‡Æ±‡Æø‡ÆØ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ§‡ØÅ\"},\n    {\"incorrect\": \"‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡ØÅ‡Æö‡Øç‡Æö‡ØÇ‡Æ¥‡Æ≤‡Øç ‡Æ™‡Ææ‡Æ§‡ØÅ‡Æï‡Ææ‡Æ™‡Øç‡Æ™‡ØÅ ‡ÆÆ‡Æø‡Æï ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç\", \"corrected\": \"‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡ØÅ‡Æö‡Øç‡Æö‡ØÇ‡Æ¥‡Æ≤‡Øç ‡Æ™‡Ææ‡Æ§‡ØÅ‡Æï‡Ææ‡Æ™‡Øç‡Æ™‡ØÅ ‡ÆÆ‡Æø‡Æï ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç\"},\n    {\"incorrect\": \"‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æµ‡Æø‡Æ∞‡Øà‡Æµ‡Æø‡Æ≤‡Øç ‡Æö‡ØÜ‡Æ©‡Øç‡Æ±‡ØÅ ‡Æµ‡Æø‡Æü‡Øç‡Æü‡Ææ‡Æ©‡Øç\", \"corrected\": \"‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æµ‡Æø‡Æ∞‡Øà‡Æµ‡Ææ‡Æï ‡Æö‡ØÜ‡Æ©‡Øç‡Æ±‡ØÅ ‡Æµ‡Æø‡Æü‡Øç‡Æü‡Ææ‡Æ©‡Øç\"},\n    {\"incorrect\": \"‡Æ™‡Æü‡Æø‡Æ™‡Øç‡Æ™‡ØÅ ‡ÆÆ‡ØÅ‡Æü‡Æø‡Æ§‡Øç‡Æ§ ‡Æµ‡Øá‡Æ≤‡Øà ‡Æ™‡ØÜ‡Æ±‡Øç‡Æ±‡Ææ‡Æ©‡Øç\", \"corrected\": \"‡Æ™‡Æü‡Æø‡Æ™‡Øç‡Æ™‡ØÅ ‡ÆÆ‡ØÅ‡Æü‡Æø‡Æ§‡Øç‡Æ§‡ØÅ ‡Æµ‡Øá‡Æ≤‡Øà ‡Æ™‡ØÜ‡Æ±‡Øç‡Æ±‡Ææ‡Æ©‡Øç\"},\n    {\"incorrect\": \"‡Æ®‡Øá‡Æ∞‡ÆÆ‡Øç ‡Æö‡ØÜ‡Æ≤‡Æµ‡Æ¥‡Æø‡Æï‡Øç‡Æï ‡ÆÆ‡Æø‡Æï ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç\", \"corrected\": \"‡Æ®‡Øá‡Æ∞‡ÆÆ‡Øç ‡Æö‡ØÜ‡Æ≤‡Æµ‡Æ¥‡Æø‡Æï‡Øç‡Æï ‡ÆÆ‡Æø‡Æï ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç\"},\n    {\"incorrect\": \"‡ÆÆ‡Æï‡Æø‡Æ¥‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æâ‡Æ£‡Æ∞‡Øç‡Æµ‡ØÅ ‡ÆÆ‡Æ©‡Æ§‡Øà ‡Æâ‡Æ±‡Øç‡Æö‡Ææ‡Æï‡ÆÆ‡Øç\", \"corrected\": \"‡ÆÆ‡Æï‡Æø‡Æ¥‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æâ‡Æ£‡Æ∞‡Øç‡Æµ‡ØÅ ‡ÆÆ‡Æ©‡Æ§‡Øà ‡Æâ‡Æ±‡Øç‡Æö‡Ææ‡Æï‡ÆÆ‡Øç\"},\n    {\"incorrect\": \"‡ÆÆ‡Æ¥‡Øà ‡Æï‡Ææ‡Æ≤‡Æ®‡Æø‡Æ≤‡Øà ‡Æï‡Æü‡ØÅ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æï ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ\", \"corrected\": \"‡ÆÆ‡Æ¥‡Øà‡Æï‡Øç‡Æï‡Ææ‡Æ≤ ‡Æ®‡Æø‡Æ≤‡Øà ‡Æï‡Æü‡ØÅ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æï ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ\"},\n\n    # 3-word sentences (2 samples)\n    {\"incorrect\": \"‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡ØÜ‡Æ±‡Øç‡Æ± ‡ÆÆ‡Æ§‡Æø‡Æ™‡Øç‡Æ™‡ØÜ‡Æ£‡Øç\", \"corrected\": \"‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡ØÜ‡Æ±‡Øç‡Æ± ‡ÆÆ‡Æ§‡Æø‡Æ™‡Øç‡Æ™‡ØÜ‡Æ£‡Øç‡Æï‡Æ≥‡Øç\"},\n    {\"incorrect\": \"‡Æö‡Æø‡Æ±‡Æ®‡Øç‡Æ§ ‡Æá‡Æö‡Øà ‡Æ™‡Ææ‡Æü‡Æ≤‡Øç\", \"corrected\": \"‡Æö‡Æø‡Æ±‡Æ®‡Øç‡Æ§ ‡Æá‡Æö‡Øà‡Æ™‡Øç ‡Æ™‡Ææ‡Æü‡Æ≤‡Øç\"},\n]\n\n\ntotal_bleu_score = 0.0\ntotal_gleu_score = 0.0\nreferences = []  # Store references for TER\nhypotheses = []  # Store hypotheses for TER\n\nnum_samples = len(tamil_samples)\n\nfor sample in tamil_samples:\n    incorrect_sentence = sample[\"incorrect\"]\n    corrected_sentence = sample[\"corrected\"]\n\n    # Tokenize the incorrect sentence\n    inputs = tokenizer(incorrect_sentence, return_tensors=\"pt\")\n\n    # Move inputs to the same device as the model (GPU if available)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n\n    # Generate the predicted sentence\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            max_length=50,\n            num_beams=5,  \n            temperature=0.7,  \n            top_k=50, \n            top_p=0.9,\n            early_stopping=True\n        )\n\n    # Decode the predicted sentence\n    predicted_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Compute BLEU score\n    reference = [nltk.word_tokenize(corrected_sentence)]\n    candidate = nltk.word_tokenize(predicted_sentence)\n    bleu_score = sentence_bleu(reference, candidate, weights=(0.5, 0.5))  # Bi-gram BLEU score\n    total_bleu_score += bleu_score\n\n    # Compute GLEU score\n    gleu_score = sentence_gleu(reference, candidate)\n    total_gleu_score += gleu_score\n\n    # Store sentences for TER calculation\n    references.append([corrected_sentence])\n    hypotheses.append(predicted_sentence)\n\n    # Print results\n    print(\"\\nIncorrect Sentence:\", incorrect_sentence)\n    print(\"Corrected Sentence:\", corrected_sentence)\n    print(\"Predicted Sentence:\", predicted_sentence)\n    print(\"BLEU Score: {:.2f}\".format(bleu_score))\n    print(\"GLEU Score: {:.2f}\".format(gleu_score))\n\n# Compute the average BLEU and GLEU scores\naverage_bleu_score = total_bleu_score / num_samples\naverage_gleu_score = total_gleu_score / num_samples\n\n# Compute TER score\nter_score = sacrebleu.corpus_ter(hypotheses, references).score\n\nprint(\"\\nAverage BLEU Score: {:.2f}\".format(average_bleu_score))\nprint(\"Average GLEU Score: {:.2f}\".format(average_gleu_score))\nprint(\"TER Score: {:.2f}\".format(ter_score))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:46:53.315329Z","iopub.execute_input":"2025-03-13T14:46:53.315668Z","iopub.status.idle":"2025-03-13T14:46:57.499276Z","shell.execute_reply.started":"2025-03-13T14:46:53.315642Z","shell.execute_reply":"2025-03-13T14:46:57.498594Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nIncorrect Sentence: ‡Æµ‡Ææ‡Æï‡Øç‡Æï‡Ææ‡Æ≥‡Æ∞‡Øç ‡Æµ‡Æ¥‡Æô‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ§‡ØÅ ‡ÆÖ‡Æ∞‡Æö‡ØÅ ‡ÆÖ‡Æ±‡Æø‡Æµ‡Æø‡Æ™‡Øç‡Æ™‡ØÅ\nCorrected Sentence: ‡Æµ‡Ææ‡Æï‡Øç‡Æï‡Ææ‡Æ≥‡Æ∞‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æµ‡Æ¥‡Æô‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ§‡ØÅ ‡ÆÖ‡Æ∞‡Æö‡ØÅ ‡ÆÖ‡Æ±‡Æø‡Æµ‡Æø‡Æ™‡Øç‡Æ™‡ØÅ\nPredicted Sentence: ‡Æµ‡Ææ‡Æï‡Øç‡Æï‡Ææ‡Æ≥‡Æ∞‡Øç ‡Æµ‡Æ¥‡Æô‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ§‡ØÅ ‡ÆÖ‡Æ∞‡Æö‡ØÅ ‡ÆÖ‡Æ±‡Æø‡Æµ‡Æø‡Æ™‡Øç‡Æ™‡ØÅ.\nBLEU Score: 0.55\nGLEU Score: 0.43\n\nIncorrect Sentence: ‡ÆÖ‡Æ±‡Æø‡Æµ‡Æø‡ÆØ‡Æ≤‡Øç ‡Æ§‡Øä‡Æ¥‡Æø‡Æ≤‡Øç‡Æ®‡ØÅ‡Æü‡Øç‡Æ™‡ÆÆ‡Øç ‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç\nCorrected Sentence: ‡ÆÖ‡Æ±‡Æø‡Æµ‡Æø‡ÆØ‡Æ≤‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æ§‡Øä‡Æ¥‡Æø‡Æ≤‡Øç‡Æ®‡ØÅ‡Æü‡Øç‡Æ™ ‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç\nPredicted Sentence: ‡ÆÖ‡Æ±‡Æø‡Æµ‡Æø‡ÆØ‡Æ≤‡Øç ‡Æ§‡Øä‡Æ¥‡Æø‡Æ≤‡Øç‡Æ®‡ØÅ‡Æü‡Øç‡Æ™‡ÆÆ‡Øç ‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç\nBLEU Score: 0.39\nGLEU Score: 0.29\n\nIncorrect Sentence: ‡ÆÖ‡Æ©‡Øç‡Æ™‡ØÅ ‡Æ™‡Ææ‡Æ∞‡Ææ‡Æü‡Øç‡Æü‡ØÅ ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Ææ‡Æ∞‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Æø‡Æ≤‡Øà‡ÆØ‡Æø‡Æ≤‡Øç\nCorrected Sentence: ‡ÆÖ‡Æ©‡Øç‡Æ™‡ØÅ‡Æ™‡Øç ‡Æ™‡Ææ‡Æ∞‡Ææ‡Æü‡Øç‡Æü‡ØÅ ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Ææ‡Æ∞‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Æø‡Æ≤‡Øà‡ÆØ‡Æø‡Æ≤‡Øç\nPredicted Sentence: ‡ÆÖ‡Æ©‡Øç‡Æ™‡ØÅ ‡Æ™‡Ææ‡Æ∞‡Ææ‡Æü‡Øç‡Æü‡ØÅ ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Ææ‡Æ∞‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Æø‡Æ≤‡Øà‡ÆØ‡Æø‡Æ≤‡Øç.\nBLEU Score: 0.63\nGLEU Score: 0.56\n\nIncorrect Sentence: ‡ÆÖ‡Æµ‡Æö‡Æ∞ ‡Æ®‡Æü‡Æµ‡Æü‡Æø‡Æï‡Øç‡Æï ‡Æé‡Æü‡ØÅ‡Æï‡Øç‡Æï ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç\nCorrected Sentence: ‡ÆÖ‡Æµ‡Æö‡Æ∞ ‡Æ®‡Æü‡Æµ‡Æü‡Æø‡Æï‡Øç‡Æï‡Øà ‡Æé‡Æü‡ØÅ‡Æï‡Øç‡Æï ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç\nPredicted Sentence: ‡ÆÖ‡Æµ‡Æö‡Æ∞ ‡Æ®‡Æü‡Æµ‡Æü‡Æø‡Æï‡Øç‡Æï ‡Æé‡Æü‡ØÅ‡Æï‡Øç‡Æï ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç.\nBLEU Score: 0.39\nGLEU Score: 0.29\n\nIncorrect Sentence: ‡Æ®‡Æü‡Æø‡Æï‡Æ∞‡Øç ‡Æ™‡Øá‡Æö‡Æø‡Æ©‡Ææ‡Æ∞‡Øç ‡Æ∞‡Æö‡Æø‡Æï‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡ØÇ‡Æü‡Øç‡Æü‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç\nCorrected Sentence: ‡Æ®‡Æü‡Æø‡Æï‡Æ∞‡Øç ‡Æ™‡Øá‡Æö‡Æø‡Æ©‡Ææ‡Æ∞‡Øç ‡Æ∞‡Æö‡Æø‡Æï‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡ØÇ‡Æü‡Øç‡Æü‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç\nPredicted Sentence: ‡Æ®‡Æü‡Æø‡Æï‡Æ∞‡Øç ‡Æ™‡Øá‡Æö‡Æø‡Æ©‡Ææ‡Æ∞‡Øç ‡Æ∞‡Æö‡Æø‡Æï‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡ØÇ‡Æü‡Øç‡Æü‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡Æµ‡Æø‡Æ≥‡Øà‡ÆØ‡Ææ‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Øã‡ÆÆ‡Øç.\nBLEU Score: 0.63\nGLEU Score: 0.56\n\nIncorrect Sentence: ‡ÆÆ‡Æ¥‡Øà ‡Æï‡Ææ‡Æ∞‡Æ£‡ÆÆ‡Øç ‡Æµ‡ØÜ‡Æ≥‡Øç‡Æ≥‡ÆÆ‡Øç ‡Æè‡Æ±‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ§‡ØÅ\nCorrected Sentence: ‡ÆÆ‡Æ¥‡Øà ‡Æï‡Ææ‡Æ∞‡Æ£‡ÆÆ‡Ææ‡Æï ‡Æµ‡ØÜ‡Æ≥‡Øç‡Æ≥‡ÆÆ‡Øç ‡Æè‡Æ±‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ§‡ØÅ\nPredicted Sentence: ‡ÆÆ‡Æ¥‡Øà ‡Æï‡Ææ‡Æ∞‡Æ£‡ÆÆ‡Øç ‡Æµ‡ØÜ‡Æ≥‡Øç‡Æ≥‡ÆÆ‡Øç ‡Æè‡Æ±‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ§‡ØÅ.\nBLEU Score: 0.39\nGLEU Score: 0.29\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"\nIncorrect Sentence: ‡Æ®‡Ææ‡Æü‡ØÅ ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Øá‡Æ±‡Øç‡Æ± ‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æá‡Æ©‡Øç‡Æ±‡Æø‡ÆØ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ§‡ØÅ\nCorrected Sentence: ‡Æ®‡Ææ‡Æü‡ØÅ ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Øá‡Æ±‡Øç‡Æ±‡ÆÆ‡Øç ‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æá‡Æ©‡Øç‡Æ±‡Æø‡ÆØ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ§‡ØÅ\nPredicted Sentence: ‡Æ®‡Ææ‡Æü‡ØÅ ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Øá‡Æ±‡Øç‡Æ±‡Æ§‡Øç‡Æ§‡Øà ‡Æá‡Æ©‡Øç‡Æ±‡Æø‡ÆØ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ§‡Ææ‡Æ©‡Øç ‡Æö‡Ææ‡Æ™‡Øç‡Æ™‡Æø‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Ææ‡Æ©‡Øç.\nBLEU Score: 0.45\nGLEU Score: 0.07\n\nIncorrect Sentence: ‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡ØÅ‡Æö‡Øç‡Æö‡ØÇ‡Æ¥‡Æ≤‡Øç ‡Æ™‡Ææ‡Æ§‡ØÅ‡Æï‡Ææ‡Æ™‡Øç‡Æ™‡ØÅ ‡ÆÆ‡Æø‡Æï ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç\nCorrected Sentence: ‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡ØÅ‡Æö‡Øç‡Æö‡ØÇ‡Æ¥‡Æ≤‡Øç ‡Æ™‡Ææ‡Æ§‡ØÅ‡Æï‡Ææ‡Æ™‡Øç‡Æ™‡ØÅ ‡ÆÆ‡Æø‡Æï ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç\nPredicted Sentence: ‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡ØÅ‡Æö‡Øç‡Æö‡ØÇ‡Æ¥‡Æ≤‡Øç ‡Æ™‡Ææ‡Æ§‡ØÅ‡Æï‡Ææ‡Æ™‡Øç‡Æ™‡ØÅ ‡ÆÆ‡Æø‡Æï ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç.\nBLEU Score: 0.77\nGLEU Score: 0.71\n\nIncorrect Sentence: ‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æµ‡Æø‡Æ∞‡Øà‡Æµ‡Æø‡Æ≤‡Øç ‡Æö‡ØÜ‡Æ©‡Øç‡Æ±‡ØÅ ‡Æµ‡Æø‡Æü‡Øç‡Æü‡Ææ‡Æ©‡Øç\nCorrected Sentence: ‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æµ‡Æø‡Æ∞‡Øà‡Æµ‡Ææ‡Æï ‡Æö‡ØÜ‡Æ©‡Øç‡Æ±‡ØÅ ‡Æµ‡Æø‡Æü‡Øç‡Æü‡Ææ‡Æ©‡Øç\nPredicted Sentence: ‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æµ‡Æø‡Æ∞‡Øà‡Æµ‡Æø‡Æ≤‡Øç ‡Æö‡ØÜ‡Æ©‡Øç‡Æ±‡ØÅ ‡Æµ‡Æø‡Æü‡Øç‡Æü‡Ææ‡Æ©‡Øç.\nBLEU Score: 0.39\nGLEU Score: 0.29\n\nIncorrect Sentence: ‡Æ™‡Æü‡Æø‡Æ™‡Øç‡Æ™‡ØÅ ‡ÆÆ‡ØÅ‡Æü‡Æø‡Æ§‡Øç‡Æ§ ‡Æµ‡Øá‡Æ≤‡Øà ‡Æ™‡ØÜ‡Æ±‡Øç‡Æ±‡Ææ‡Æ©‡Øç\nCorrected Sentence: ‡Æ™‡Æü‡Æø‡Æ™‡Øç‡Æ™‡ØÅ ‡ÆÆ‡ØÅ‡Æü‡Æø‡Æ§‡Øç‡Æ§‡ØÅ ‡Æµ‡Øá‡Æ≤‡Øà ‡Æ™‡ØÜ‡Æ±‡Øç‡Æ±‡Ææ‡Æ©‡Øç\nPredicted Sentence: ‡Æ™‡Æü‡Æø ‡Æ™‡Æü‡Æø‡Æ§‡Øç‡Æ§ ‡ÆÆ‡ØÅ‡Æü‡Æø‡Æ§‡Øç‡Æ§ ‡Æµ‡Øá‡Æ≤‡Øà ‡Æ™‡ØÜ‡Æ±‡Øç‡Æ±‡Ææ‡Æ©‡Øç.\nBLEU Score: 0.26\nGLEU Score: 0.17\n\nIncorrect Sentence: ‡Æ®‡Øá‡Æ∞‡ÆÆ‡Øç ‡Æö‡ØÜ‡Æ≤‡Æµ‡Æ¥‡Æø‡Æï‡Øç‡Æï ‡ÆÆ‡Æø‡Æï ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç\nCorrected Sentence: ‡Æ®‡Øá‡Æ∞‡ÆÆ‡Øç ‡Æö‡ØÜ‡Æ≤‡Æµ‡Æ¥‡Æø‡Æï‡Øç‡Æï ‡ÆÆ‡Æø‡Æï ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç\nPredicted Sentence: ‡Æ®‡Øá‡Æ∞‡ÆÆ‡Øç ‡Æö‡ØÜ‡Æ≤‡Æµ‡Æ¥‡Æø‡Æï‡Øç‡Æï ‡ÆÆ‡Æø‡Æï ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Øç.\nBLEU Score: 0.77\nGLEU Score: 0.71\n\nIncorrect Sentence: ‡ÆÆ‡Æï‡Æø‡Æ¥‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æâ‡Æ£‡Æ∞‡Øç‡Æµ‡ØÅ ‡ÆÆ‡Æ©‡Æ§‡Øà ‡Æâ‡Æ±‡Øç‡Æö‡Ææ‡Æï‡ÆÆ‡Øç\nCorrected Sentence: ‡ÆÆ‡Æï‡Æø‡Æ¥‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æâ‡Æ£‡Æ∞‡Øç‡Æµ‡ØÅ ‡ÆÆ‡Æ©‡Æ§‡Øà ‡Æâ‡Æ±‡Øç‡Æö‡Ææ‡Æï‡ÆÆ‡Øç\nPredicted Sentence: ‡ÆÆ‡Æï‡Æø‡Æ¥‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æâ‡Æ£‡Æ∞‡Øç‡Æµ‡ØÅ ‡ÆÆ‡Æ©‡Æ§‡Øà ‡Æâ‡Æ±‡Øç‡Æö‡Ææ‡Æï‡ÆÆ‡Øç ‡Æ™‡Æü‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Ææ‡Æ≥‡Øç.\nBLEU Score: 0.63\nGLEU Score: 0.56\n\nIncorrect Sentence: ‡ÆÆ‡Æ¥‡Øà ‡Æï‡Ææ‡Æ≤‡Æ®‡Æø‡Æ≤‡Øà ‡Æï‡Æü‡ØÅ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æï ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ\nCorrected Sentence: ‡ÆÆ‡Æ¥‡Øà‡Æï‡Øç‡Æï‡Ææ‡Æ≤ ‡Æ®‡Æø‡Æ≤‡Øà ‡Æï‡Æü‡ØÅ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æï ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ\nPredicted Sentence: ‡ÆÆ‡Æ¥‡Øà ‡Æï‡Ææ‡Æ≤‡Æ®‡Æø‡Æ≤‡Øà ‡Æï‡Æü‡ØÅ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æï ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ.\nBLEU Score: 0.32\nGLEU Score: 0.21\n\nIncorrect Sentence: ‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡ØÜ‡Æ±‡Øç‡Æ± ‡ÆÆ‡Æ§‡Æø‡Æ™‡Øç‡Æ™‡ØÜ‡Æ£‡Øç\nCorrected Sentence: ‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡ØÜ‡Æ±‡Øç‡Æ± ‡ÆÆ‡Æ§‡Æø‡Æ™‡Øç‡Æ™‡ØÜ‡Æ£‡Øç‡Æï‡Æ≥‡Øç\nPredicted Sentence: ‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡ØÜ‡Æ±‡Øç‡Æ± ‡ÆÆ‡Æ§‡Æø‡Æ™‡Øç‡Æ™‡ØÜ‡Æ£‡Øç\nBLEU Score: 0.58\nGLEU Score: 0.50\n\nIncorrect Sentence: ‡Æö‡Æø‡Æ±‡Æ®‡Øç‡Æ§ ‡Æá‡Æö‡Øà ‡Æ™‡Ææ‡Æü‡Æ≤‡Øç\nCorrected Sentence: ‡Æö‡Æø‡Æ±‡Æ®‡Øç‡Æ§ ‡Æá‡Æö‡Øà‡Æ™‡Øç ‡Æ™‡Ææ‡Æü‡Æ≤‡Øç\nPredicted Sentence: ‡Æö‡Æø‡Æ±‡Æ®‡Øç‡Æ§ ‡Æá‡Æö‡Øà ‡Æ™‡Ææ‡Æü‡Æ≤‡Øç\nBLEU Score: 0.82\nGLEU Score: 0.33\n\nAverage BLEU Score: 0.53\nAverage GLEU Score: 0.40\nTER Score: 50.00\n","output_type":"stream"}],"execution_count":13}]}